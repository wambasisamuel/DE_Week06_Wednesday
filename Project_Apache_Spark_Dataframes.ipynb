{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q-AD3sVo6cFq",
        "r2sRFJcsLkQw",
        "Dq0_sGF352b-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6vot1-PLi70"
      },
      "source": [
        "# Project - Apache Spark Dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPtkMdA_OWEf"
      },
      "source": [
        "## Pre-requisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NIjZ013OX0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d776b42e-34ae-45d1-e1f6-c0bc82bc3747"
      },
      "source": [
        "# Installing pyspark\n",
        "# ---\n",
        "#\n",
        "!pip install pyspark"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5ggFX2DOccp"
      },
      "source": [
        "# Running a local spark session\n",
        "# ---\n",
        "#\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "from pyspark import SparkFiles\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Importation and Exploration"
      ],
      "metadata": {
        "id": "q-AD3sVo6cFq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2sRFJcsLkQw"
      },
      "source": [
        "### Register the DataFrame as a Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOgI8-AELUmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a5ba67-5c53-4061-946c-8874238cc8a2"
      },
      "source": [
        "# Dataset URL: https://raw.githubusercontent.com/wambasisamuel/DE_Week06_Wednesday/main/saf_stock.csv\n",
        "\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "sqlCtx = SQLContext(sc)\n",
        "spark.sparkContext.addFile(\"https://raw.githubusercontent.com/wambasisamuel/DE_Week06_Wednesday/main/saf_stock.csv\")\n",
        "df = spark.read.options(header=True, inferSchema='True', delimiter=',', dateFormat='yyyy-mm-dd').csv(\"file://\" + SparkFiles.get(\"saf_stock.csv\"))\n",
        "\n",
        "df.createOrReplaceTempView('saf_stock')\n",
        "tables = sqlCtx.tableNames()\n",
        "print(tables)\n",
        "\n",
        "#df.show(10)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['saf_stock']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determine column names"
      ],
      "metadata": {
        "id": "Dq0_sGF352b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TpIPBeT6un0",
        "outputId": "e4ea5c0e-14e6-4a0e-c417-a5810877dd49"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations about the schema"
      ],
      "metadata": {
        "id": "KV4Ccyo06vJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpS24_sE52-E",
        "outputId": "9253dade-958e-4269-f4cb-0742eb1974c5"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: timestamp (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The first five rows"
      ],
      "metadata": {
        "id": "Z-by6kWH6_GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsRXcyoz7CPu",
        "outputId": "c3c8bf63-866d-4f58-c385-57f5af514ecb"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
            "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
            "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
            "|2012-01-03 00:00:00|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
            "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
            "|2012-01-05 00:00:00|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
            "|2012-01-06 00:00:00|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
            "|2012-01-09 00:00:00|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|\n",
            "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_3EqMX1L5K1"
      },
      "source": [
        "### Describing the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAgVUquAL_QD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "842e4ef3-fc12-4ac7-b117-83816d35ba2c"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
            "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
            "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "VHfwYXiQ7xVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format all the data to 2 decimal places i.e. format_number()\n",
        "from pyspark.sql.functions import format_number\n",
        "\n",
        "df = df.select(\"Date\",\n",
        "               format_number(\"Open\", 2).alias(\"Open\"),\n",
        "                format_number(\"High\", 2).alias(\"High\"),\n",
        "                format_number(\"Low\", 2).alias(\"Low\"),\n",
        "                format_number(\"Close\", 2).alias(\"Close\"),\n",
        "                format_number(\"Volume\", 2).alias(\"Volume\"),\n",
        "                format_number(\"Adj Close\", 2).alias(\"Adj Close\")\n",
        "                )\n",
        "df.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSXLSNjT8VJs",
        "outputId": "11615387-fdd6-40f5-9132-dc7a1b846ece"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+-----+-----+-----+-------------+---------+\n",
            "|               Date| Open| High|  Low|Close|       Volume|Adj Close|\n",
            "+-------------------+-----+-----+-----+-----+-------------+---------+\n",
            "|2012-01-03 00:00:00|59.97|61.06|59.87|60.33|12,668,800.00|    52.62|\n",
            "|2012-01-04 00:00:00|60.21|60.35|59.47|59.71| 9,593,300.00|    52.08|\n",
            "|2012-01-05 00:00:00|59.35|59.62|58.37|59.42|12,768,200.00|    51.83|\n",
            "+-------------------+-----+-----+-----+-----+-------------+---------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A new data frame with a column called HV Ratio\n",
        "df2 = df.withColumn(\"HV Ratio\",df['High']/df['Volume'])\n",
        "print(df2.select('HV Ratio').show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8hHIyrkBQV-",
        "outputId": "d5c8f0c4-c583-4da0-caef-a2fd13125331"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|HV Ratio|\n",
            "+--------+\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "|    null|\n",
            "+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n02grz6wL_2O"
      },
      "source": [
        "## Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What day had the Peak High in Price?\n",
        "peak_high_day = df.orderBy(df['High'].desc()).head(1)\n",
        "print(peak_high_day[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lmfiogqCkvC",
        "outputId": "b321b6c4-15e2-47be-ccd3-1d96f8afd9df"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2015-01-13 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the mean of the Close column?\n",
        "from pyspark.sql.functions import mean\n",
        "close_mean = df.select(mean('Close')).show()\n",
        "#print(close_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhoSoyYREt2g",
        "outputId": "80348768-2350-422d-e538-cbb0675c43c9"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|       avg(Close)|\n",
            "+-----------------+\n",
            "|72.38844992050863|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the max and min of the Volume column?\n",
        "from pyspark.sql.functions import max, min\n",
        "df.select(max(\"Volume\"), min(\"Volume\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig9q398OFR0b",
        "outputId": "03464a20-d3e2-4906-fb7a-d1b4192e09b4"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------+\n",
            "| max(Volume)|  min(Volume)|\n",
            "+------------+-------------+\n",
            "|9,994,400.00|10,010,500.00|\n",
            "+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many days was the Close lower than 60 dollars?\n",
        "close_lt_60_days = df.filter(df['Close'] < 60).count()\n",
        "print(close_lt_60_days)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekhE4AR_Gf05",
        "outputId": "4b5a604e-1c27-43b4-d8ef-7a413b533621"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What percentage of the time was the High greater than 80 dollars?\n",
        "perc = (df.filter(df['High']>80).count()/df.count()) * 100\n",
        "print(f\"{perc}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u5SqOBKHCFI",
        "outputId": "f9c8de5c-e508-4dad-81a1-6c34857b940a"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.426073131955485%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the Pearson correlation between High and Volume?\n",
        "from pyspark.sql.functions import corr\n",
        "\n",
        "df.select(corr('High','Volume')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzhN0lWkIaxu",
        "outputId": "f9d2c4f4-0dc7-42e1-ccfe-63eb024507e8"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|corr(High, Volume)|\n",
            "+------------------+\n",
            "|              null|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the max High per year?\n",
        "from pyspark.sql.functions import year\n",
        "yeardf = df.withColumn(\"Year\",year(df['Date']))\n",
        "max_df = yeardf.groupBy('Year').max()\n",
        "#print(max_df.select('Year',\"max(Year)\").show())\n",
        "\n",
        "df.withColumn(\"Year\", year(df[\"Date\"])).groupBy('Year').agg(max(\"High\").alias(\"High\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "care6C26Iyq2",
        "outputId": "d04b125e-8e49-4a63-afbc-523c83f5702a"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|Year| High|\n",
            "+----+-----+\n",
            "|2012|77.60|\n",
            "|2013|81.37|\n",
            "|2014|88.09|\n",
            "|2015|90.97|\n",
            "|2016|75.19|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the average Close for each Calendar Month\n",
        "from pyspark.sql.functions import month, avg\n",
        "\n",
        "df.withColumn(\"Month\", month(df[\"Date\"])).groupBy('Month').agg(avg(\"Close\").alias(\"Close\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISE5ygeSLbMH",
        "outputId": "99349aa7-a169-4167-a235-135a916904af"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|Month|            Close|\n",
            "+-----+-----------------+\n",
            "|   12|72.84792452830189|\n",
            "|    1|71.44801980198022|\n",
            "|    6|72.49537735849057|\n",
            "|    3|71.77794392523363|\n",
            "|    5|72.30971698113206|\n",
            "|    9|72.18411764705883|\n",
            "|    4|72.97361904761907|\n",
            "|    8|73.02981818181819|\n",
            "|    7|74.43971962616824|\n",
            "|   10|71.57854545454546|\n",
            "|   11|72.11108910891085|\n",
            "|    2|71.30680412371134|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}